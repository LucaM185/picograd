{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "debug = 1\n",
    "def mean(mylist): return sum(mylist) / len(mylist) if len(mylist) > 0 else 0\n",
    "def prod(a): return np.array(a).prod() \n",
    "\n",
    "# Tensor operations return lazybuffers, if you realize the lazybuffers, they run forward()\n",
    "\n",
    "### SHAPETRACKER ###\n",
    "\n",
    "class ShapeTracker:\n",
    "    def __init__(self, shape) -> None:\n",
    "        self.shape = shape \n",
    "        self.permutes = None\n",
    "        self.stride = [1]\n",
    "        self.pad = [0]\n",
    "\n",
    "\n",
    "    def __eq__(self, other):  # Incomplete\n",
    "        return mean([a == b for a, b in zip(self.shape, other.shape)]) == 1 \n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"t\" + str(self.shape)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        if isinstance(index, int): return self.shape[index]\n",
    "        if isinstance(index, slice): return self.shape[index]\n",
    "        # if isinstance(index, tuple): return ShapeTracker([self.shape[i] for i in index])\n",
    "        # Also handle tensor\n",
    "\n",
    "    def pop(self, axis):\n",
    "        if axis is not None: self.shape = tuple([i for n, i in enumerate(self.shape) if n != axis%len(self.shape)]) \n",
    "        else: self.shape = (1,) \n",
    "\n",
    "    def copy(self): return ShapeTracker(self.shape)\n",
    "    def flat(self): return prod(self.shape)\n",
    "\n",
    "### LAZYBUFFER ###\n",
    "\n",
    "class LazyBuffer:\n",
    "    def __init__(self) -> None:\n",
    "        self.buffer = []\n",
    "        self.grad = 0\n",
    "        # print(type(self).__name__)\n",
    "    \n",
    "    def numpy(self):\n",
    "        return self.forward()\n",
    "    \n",
    "    def debug(self):\n",
    "        print(self.buffer)\n",
    "    \n",
    "    def __add__(self, other): return Add(self, other)\n",
    "    def __radd__(self, other): return Add(other, self)\n",
    "    def __mul__(self, other): return Mul(self, other)\n",
    "    def __truediv__(self, other): return Div(self, other)\n",
    "    def __rtruediv__(self, other): return Div(other, self) # Really untested\n",
    "    def __sub__(self, other): return Add(self, -other)\n",
    "    def __matmul__(self, other): return MatMul(self, other)\n",
    "    def __neg__(self): return Mul(self, -1)\n",
    "    def __pow__(self, exp): return Pow(self, exp)\n",
    "    def __exp__(self): return Exp(self)\n",
    "\n",
    "    def sum(self, axis=None): return Sum(self, axis)\n",
    "    def mean(self, axis=None): return Sum(self, axis) * (1 / self.shapeTrack.shape[axis])  # Untested\n",
    "    def onehot(self, dict_size=None): return OneHot(self, dict_size) # NOT LAZY\n",
    "    def softmax(self): return softmax(self)  # Untested\n",
    "    def max(self, axis=None): return Max(self, axis)  # Untested\n",
    "\n",
    "    def __str__(self): return self.numpy().__str__()\n",
    "\n",
    "    def shapeTrack(self, shape):\n",
    "        self.shapeTrack = ShapeTracker(shape)\n",
    "\n",
    "### TENSOR ###\n",
    "\n",
    "class Tensor(LazyBuffer):  # Tensor is just lazybuffer that contains data \n",
    "    def __init__(self, value, shape=None) -> None:\n",
    "        self.data = np.array(value)\n",
    "        self.shape = ShapeTracker(self.data.shape) if shape is None else shape # shape is a shapeTracker\n",
    "        self.requires_grad = False\n",
    "        self.grad = 0\n",
    "\n",
    "    def forward(self):\n",
    "        return self.data\n",
    "    def backward(self, grad):  # Leaf tensors don't need to do anything\n",
    "        self.grad += grad\n",
    "\n",
    "    def numpy(self):\n",
    "        return self.data\n",
    "\n",
    "    def __getitem__(self, index):   # THIS IS NOT LAZY!!\n",
    "        if isinstance(index, slice):\n",
    "            start, stop, step = index.indices(len(self.data))\n",
    "            return Tensor(self.data[start:stop:step])\n",
    "        else:\n",
    "            return Tensor(self.data[index])\n",
    "\n",
    "\n",
    "### OPERATION TYPES ###\n",
    "\n",
    "class Unary(LazyBuffer):\n",
    "    def __init__(self, a) -> None:\n",
    "        super().__init__()\n",
    "        self.a = Tensor(a) if not isinstance(a, LazyBuffer) else a\n",
    "        self.shape = self.a.shape.copy()\n",
    "\n",
    "class Binary(LazyBuffer):\n",
    "    def __init__(self, a, b) -> None:\n",
    "        super().__init__()\n",
    "        self.a = Tensor(a) if not isinstance(a, LazyBuffer) else a\n",
    "        self.b = Tensor(b) if not isinstance(b, LazyBuffer) else b\n",
    "        self.shape = self.a.shape.copy()  \n",
    "        if self.a.shape.flat() != self.b.shape.flat() and self.a.shape.shape == (self.b.shape.shape + (self.a.shape.shape[-1],)): self.b = Adapt(self.b, self.a)  # Untested\n",
    "        # assert self.a.shape == self.b.shape, f\"Shapes {self.a.shape} and {self.b.shape} are not compatible\" \n",
    "        # This ^ doesnt take b scalars into account\n",
    "\n",
    "class Reduce(LazyBuffer):\n",
    "    def __init__(self, a, axis=None) -> None:\n",
    "        super().__init__()\n",
    "        self.grad = 0\n",
    "        self.a = Tensor(a) if not isinstance(a, LazyBuffer) else a  # Deal with shapetrack\n",
    "        self.shape = self.a.shape.copy()\n",
    "        self.axis = axis\n",
    "        self.shape.pop(axis)\n",
    "        # self.shape = 1 if axis is None else (elm for i, elm in enumerate(self.a.shape) if i != axis)\n",
    "\n",
    "class Broadcast(LazyBuffer):\n",
    "    def __init__(self, a, b) -> None:\n",
    "        super().__init__()\n",
    "        self.a = Tensor(a) if not isinstance(a, LazyBuffer) else a\n",
    "        self.b = Tensor(b) if not isinstance(b, LazyBuffer) else b\n",
    "        self.shape = ShapeTracker(self.a.shape.shape + (self.b.forward(),))\n",
    "\n",
    "### OPERATIONS ###\n",
    "\n",
    "def softmax(tensor): # safe softmax\n",
    "    # exp = Exp(tensor - tensor.max())\n",
    "    return Exp(tensor - tensor.max()) / (Exp(tensor - tensor.max()).sum(axis=-1) )\n",
    "\n",
    "def OneHot(tensor, dict_size):\n",
    "    if dict_size is None: dict_size = int(tensor.data.max()) + 1\n",
    "    result = np.zeros((tensor.shape[0], dict_size))\n",
    "    result[np.arange(tensor.shape[0]), np.int32(tensor.data)] = 1\n",
    "    return Tensor(result)    \n",
    "\n",
    "class Exp(Unary):\n",
    "    def forward(self):  # Exp\n",
    "        self.data = np.exp(self.a.forward())\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad * self.data)\n",
    "\n",
    "\n",
    "class Div(Binary):\n",
    "    def forward(self):  # a / b\n",
    "        self.data = self.a.forward() / self.b.forward()\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad / self.b.data)\n",
    "        self.b.backward(grad * -self.a.data / (self.b.data ** 2)) \n",
    "\n",
    "class Pow(Binary):\n",
    "    def forward(self):  # a ** b\n",
    "        self.data = self.a.forward() ** self.b.forward() \n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad * self.b.data * self.a.data ** (self.b.data - 1))\n",
    "        self.b.backward(grad * self.data * np.log(self.a.data))\n",
    "\n",
    "class Add(Binary):\n",
    "    def forward(self):  # a + b\n",
    "        self.data = self.a.forward() + self.b.forward()\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad)\n",
    "        self.b.backward(grad)\n",
    "\n",
    "class Mul(Binary):\n",
    "    def forward(self):\n",
    "        self.data = self.a.forward() * self.b.forward()\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad * self.b.data)\n",
    "        self.b.backward(grad * self.a.data)\n",
    "\n",
    "class MatMul(Binary):\n",
    "    def __init__(self, a, b) -> None:\n",
    "        super().__init__(a, b)\n",
    "        self.shape = ShapeTracker(self.a.shape.shape[:-1] + self.b.shape.shape[1:])\n",
    "\n",
    "    def forward(self):\n",
    "        self.data = self.a.forward() @ self.b.forward()\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad @ self.b.data.T)\n",
    "        self.b.backward(self.a.data.T @ grad)\n",
    "\n",
    "\n",
    "class Sum(Reduce): \n",
    "    def forward(self):\n",
    "        self.data = np.sum(self.a.forward(), axis=self.axis)\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=Tensor((1,))):\n",
    "        self.grad += grad\n",
    "        # self.a.backward(np.repeat(grad, (self.a.shape.flat()//self.grad.shape.flat()), axis=self.axis).reshape(*self.a.shape)) # Untested\n",
    "        self.a.backward(Adapt(self.grad, self.a).numpy())\n",
    "\n",
    "class Mean(Reduce):  # Untested\n",
    "    def forward(self):\n",
    "        self.data = np.mean(self.a.forward(), axis=self.axis)\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad * np.ones(self.a.shape) / self.a.shapeTrack.shape[self.axis])\n",
    "\n",
    "class Max(Reduce): \n",
    "    def forward(self):\n",
    "        self.data = np.max(self.a.forward(), axis=self.axis)\n",
    "        return self.data\n",
    "    \n",
    "    def backward(self, grad=1):  # WTF is the gradient of max? Definetly untested\n",
    "        self.grad += grad\n",
    "        self.a.backward(grad * (self.a.data == self.data))\n",
    "\n",
    "# class Repeat(Broadcast):\n",
    "#     def forward(self):  # todo - this is far more complex than expected \n",
    "#         self.data = np.repeat(self.a.forward(), self.b.forward(), axis=-1).reshape(*self.a.forward().shape, self.b.forward())\n",
    "#         return self.data\n",
    "\n",
    "#     def backward(self, grad=1):\n",
    "#         self.grad += grad\n",
    "#         self.a.backward(np.sum(grad, axis=-1))\n",
    "#         self.b.backward(np.sum(grad * self.a.data, axis=-1))\n",
    "\n",
    "class Adapt(Broadcast):\n",
    "    def forward(self):\n",
    "        # given a tensor and b shape, adapt a to b\n",
    "        self.original_shape = self.a.shape\n",
    "        times_smaller = self.b.shape.flat() // self.a.shape.flat()\n",
    "        self.data = np.repeat(self.a.forward(), times_smaller).reshape(*self.b.shape)\n",
    "        return self.data\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        self.grad += grad\n",
    "        self.a.backward(np.sum(grad, axis=-1).reshape(self.original_shape))\n",
    "        \n",
    "\n",
    "class Rand(Tensor):\n",
    "    def __init__(self, shape) -> None:\n",
    "        self.data = np.random.rand(*shape)\n",
    "        super().__init__(self.data)\n",
    "\n",
    "class Randn(Tensor):\n",
    "    def __init__(self, shape) -> None:\n",
    "        self.data = np.random.randn(*shape)\n",
    "        super().__init__(self.data)\n",
    "\n",
    "class ReadCSV(Tensor):\n",
    "    def __init__(self, path) -> None:\n",
    "        # assume first line is the headerRepe\n",
    "        self.data = np.genfromtxt(path, delimiter=',', skip_header=1)\n",
    "        super().__init__(self.data)\n",
    "\n",
    "### MODULES ###\n",
    "\n",
    "class Module():\n",
    "    pass\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Randn((in_features, out_features)) * (1 / np.sqrt(in_features))\n",
    "        self.bias = Randn((1, out_features)) * (1 / np.sqrt(in_features)) # double check the 1 in (1, out_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self.weight.grad += self.x.T @ grad\n",
    "        self.bias.grad += grad.sum(axis=0)\n",
    "        return grad @ self.weight.T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19168\\2150821997.py:174: RuntimeWarning: invalid value encountered in log\n",
      "  self.b.backward(grad * self.data * np.log(self.a.data))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.43180922, -1.50536969, -0.26780839],\n",
       "       [-0.43217523, -0.04291346, -0.05979272],\n",
       "       [ 1.74754208, -0.40638953, -0.24657842],\n",
       "       [-1.12321174, -0.81741158,  0.62361347]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = Rand((4, 3))\n",
    "r2 = Rand((4, 3))\n",
    "diff = (r1 - r2)\n",
    "diffs = diff ** 2\n",
    "r = (diffs).sum() \n",
    "r.numpy()\n",
    "\n",
    "r.backward()\n",
    "r1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues and notes\n",
    "\n",
    "Repeat is not needed, numpy automatically broadcasts things, but fine for now <br>\n",
    "--> actually repeat is needed because of backwards, and it's far more complex than expected <br>\n",
    "Not sure about the laziness of Linear <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be cool to run on MNIST\n",
    "import pickle\n",
    "\n",
    "# dataset = ReadCSV('mnist_train.csv')\n",
    "# with open('mnist.pickle', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../mnist.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.67692624, 0.67692624, 0.67692624, 0.67692624],\n",
       "        [0.67692624, 0.67692624, 0.67692624, 0.67692624],\n",
       "        [0.67692624, 0.67692624, 0.67692624, 0.67692624]],\n",
       "\n",
       "       [[0.46993016, 0.46993016, 0.46993016, 0.46993016],\n",
       "        [0.46993016, 0.46993016, 0.46993016, 0.46993016],\n",
       "        [0.46993016, 0.46993016, 0.46993016, 0.46993016]],\n",
       "\n",
       "       [[0.85627304, 0.85627304, 0.85627304, 0.85627304],\n",
       "        [0.85627304, 0.85627304, 0.85627304, 0.85627304],\n",
       "        [0.85627304, 0.85627304, 0.85627304, 0.85627304]],\n",
       "\n",
       "       [[0.97664564, 0.97664564, 0.97664564, 0.97664564],\n",
       "        [0.97664564, 0.97664564, 0.97664564, 0.97664564],\n",
       "        [0.97664564, 0.97664564, 0.97664564, 0.97664564]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = Adapt(Rand((4,)), Rand((4, 3, 4)))\n",
    "ad.numpy()\n",
    "ad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7,10) (4,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m n1 \u001b[38;5;241m=\u001b[39m fc1(images)\n\u001b[0;32m      8\u001b[0m pred \u001b[38;5;241m=\u001b[39m n1\u001b[38;5;241m.\u001b[39msoftmax()\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m mse \u001b[38;5;241m=\u001b[39m ((pred \u001b[38;5;241m-\u001b[39m labels) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(mse\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m, in \u001b[0;36mLazyBuffer.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 178\u001b[0m, in \u001b[0;36mAdd.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# a + b\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7,10) (4,6) "
     ]
    }
   ],
   "source": [
    "images = dataset[:7, 1:]\n",
    "labels = dataset[:4, 0].onehot()\n",
    "\n",
    "fc1 = Linear(784, 10)\n",
    "\n",
    "n1 = fc1(images)\n",
    "\n",
    "pred = n1.softmax()\n",
    "print((pred - labels).numpy())\n",
    "mse = ((pred - labels) ** 2).sum() \n",
    "print(mse.numpy())\n",
    "\n",
    "mse.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109813.15396756928\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torchdata = torch.tensor(dataset.data)\n",
    "timages = torchdata[:, 1:].float()\n",
    "tlabels = torchdata[:, 0].long()\n",
    "\n",
    "w1 = torch.randn(784, 10, requires_grad=True).float()\n",
    "b1 = torch.randn(1, 10, requires_grad=True).float()\n",
    "\n",
    "torchmse = torch.nn.functional.mse_loss(torch.nn.functional.softmax(timages @ w1 + b1, dim=-1), torch.nn.functional.one_hot(tlabels, 10))\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.32357242 1.29060336 1.17901193 1.04738572 2.06071516 2.01357728\n",
      "  1.07164179 2.26901126 2.6726729  1.94057902]]\n",
      "[17.86877084]\n",
      "[[0.13003538 0.07222676 0.0659817  0.05861543 0.11532495 0.11268695\n",
      "  0.05997289 0.12698194 0.14957229 0.10860171]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax testing\n",
    "n = Rand((1, 10))\n",
    "print(Exp(n))\n",
    "print(Exp(n).sum(1))\n",
    "print(n.softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 117.61452441, -111.65694779,  107.36385164, ...,   53.58838391,\n",
       "         106.69311709,  -83.32522846],\n",
       "       [  89.56216287,  -13.8187122 ,  107.26697021, ...,   73.73735931,\n",
       "         117.12209774,  -11.48190668],\n",
       "       [  44.85184525,   25.15065779,  -37.71456661, ...,   40.05536546,\n",
       "         -36.57610678, -104.04146739],\n",
       "       ...,\n",
       "       [ 141.54043248,    9.65783841,   -1.56395171, ...,  163.73343039,\n",
       "         -74.13833273,  -36.1691029 ],\n",
       "       [  25.13424445,   18.97431544,  107.04201011, ...,   23.80922902,\n",
       "          28.84876724, -122.98191804],\n",
       "       [  76.78514152,  -18.86484713,   68.92965517, ...,  -11.26740982,\n",
       "          64.50984431,  -91.08108858]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4965278 , 1.49941769],\n",
       "       [1.75843499, 1.47767739]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear(3, 2).forward(Rand((2, 3))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(3, 2, 3)\n",
    "conv.forward(Rand((2, 3, 4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Add object at 0x0000021235AE07F0>\n",
      "<__main__.Mul object at 0x0000021235AE00D0>\n",
      "<__main__.Add object at 0x0000021235AE0280>\n",
      "<__main__.Sum object at 0x0000021235AE0160>\n",
      "256\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "[[11. 14.]\n",
      " [17. 20.]]\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.array([[1, 2], [3, 4]]))\n",
    "b = Tensor(np.array([[5, 6], [7, 8]]))\n",
    "\n",
    "c = a + b\n",
    "d = c * b\n",
    "e = d + 3\n",
    "f = e.sum()\n",
    "\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "\n",
    "print(f.numpy())\n",
    "f.backward()\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "print(d.grad)\n",
    "print(e.grad)\n",
    "print(f.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]], grad_fn=<AddBackward0>)\n",
      "tensor([[30., 48.],\n",
      "        [70., 96.]], grad_fn=<MulBackward0>)\n",
      "tensor([[33., 51.],\n",
      "        [73., 99.]], grad_fn=<AddBackward0>)\n",
      "tensor(256., grad_fn=<SumBackward0>)\n",
      "-------------------\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[11., 14.],\n",
      "        [17., 20.]])\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.array([[1, 2], [3, 4]])).requires_grad_()\n",
    "b = Tensor(np.array([[5, 6], [7, 8]])).requires_grad_()\n",
    "\n",
    "c = a + b\n",
    "d = c * b\n",
    "e = d + 3\n",
    "f = e.sum()\n",
    "\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "# print(c.grad)\n",
    "# print(d.grad)\n",
    "# print(e.grad)\n",
    "# print(f.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
